{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture\n",
    "\n",
    "First layer: concatenated fasttext and glove twitter embeddings. Fasttext vector is used by itself if there is no glove vector but not the other way around. Words without word vectors are replaced with a word vector for a word \"something\". Also, I added additional value that was set to 1 if a word was written in all capital letters and 0 otherwise.\n",
    "\n",
    "Second layer: SpatialDropout1D(0.5)\n",
    "\n",
    "Third layer: Bidirectional CuDNNLSTM with a kernel size 40. I found out that LSTM as a first layer works better than GRU.\n",
    "\n",
    "Fourth layer: Bidirectional CuDNNGRU with a kernel size 40.\n",
    "\n",
    "Fifth layer: A concatenation of the last state, maximum pool, average pool and two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
    "\n",
    "Sixth layer: output dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52644\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        hidden_size = 60\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.5)\n",
    "        self.lstm = nn.GRU(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size*2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size*2, maxlen)\n",
    "        \n",
    "        self.linear = nn.Linear(480, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #1. embedding\n",
    "        h_embedding = self.embedding(x)\n",
    "        \n",
    "        #2. spatial dropout to prevent over-fitting\n",
    "#         https://stackoverflow.com/questions/50393666/how-to-understand-spatialdropout1d-and-when-to-use-it\n",
    "        h_embedding = torch.squeeze(self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "#       3. Bidirectional LSTM\n",
    "# https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        \n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
